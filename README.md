# LEGO Assembly Vision-Based RAG System

**Complete AI-Powered LEGO Assembly Assistant with Computer Vision**

Production-ready system combining Vision-Language Models (VLMs) for manual processing with Retrieval-Augmented Generation (RAG) and **Computer Vision-Based Assembly State Analysis** for intelligent, state-aware guidance.

---

## ğŸ¯ System Overview

This system provides end-to-end LEGO assembly assistance through an integrated workflow:

### **Phase 1: Manual Processing** (Automatic)
- **ğŸ“„ Phase 0 - Document Understanding**: Smart page filtering to remove covers, ads, and inventory
- **ğŸ§  Phase 1 - Context-Aware Extraction**: VLM extraction with sliding window + long-term memory
- VLM-based step extraction from PDF instruction manuals
- 3D plan generation with spatial reasoning
- Dependency graph construction
- **ğŸ§  Hierarchical Assembly Graph**: Parts â†’ Subassemblies â†’ Model structure (enhanced with context hints)
- Part database integration with Rebrickable

### **Phase 2: Multimodal RAG Ingestion** â­ Automatic
- **ğŸ¨ Multimodal Embeddings**: Text + diagram descriptions for better visual retrieval
- **Automatic Vector Store Population**: Seamlessly integrated with Phase 1
- **Progress Checkpointing**: Resume interrupted processing automatically

### **Phase 3: RAG Backend & Frontend**
- **ğŸ¤– LLM Query Augmentation**: Understands vague queries like "What's next?"
- **ğŸ“¸ Computer Vision State Analysis**: Upload photos to track progress automatically
- **ğŸ” VLM-based Part Detection**: Identifies visible parts, colors, and connections
- **ğŸ“Š Progress Mapping**: Compares detected state with expected plan
- **âš ï¸ Error Detection**: Identifies missing parts and incorrect placements
- **ğŸ’¡ Intelligent Guidance**: Generates next-step instructions based on current state
- **ğŸ§  Graph-Enhanced Retrieval**: Combines structural and semantic search
- **ğŸ—ï¸ Subassembly Detection**: Recognizes completed structures from photos
- **ğŸ“ˆ Hierarchical Progress Tracking**: Understands assembly stages via graph
- ChromaDB vector database for semantic search
- Qwen/DeepSeek/Moonshot LLM integration
- FastAPI REST API with 20+ endpoints
- Next.js 14 web application with TypeScript

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Frontend (Next.js)                      â”‚
â”‚  Manual Selector â”‚ Step Navigator               â”‚
â”‚  Text Chat â”‚ Photo Upload & Analysis            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Backend (FastAPI)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Vision Analysis Pipeline                â”‚   â”‚
â”‚  â”‚  Photos â†’ VLM â†’ State Comparison â†’       â”‚   â”‚
â”‚  â”‚  Progress Mapping â†’ Guidance Generation  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAG Pipeline (Text Queries)             â”‚   â”‚
â”‚  â”‚  Retrieval â†’ Context â†’ LLM               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  ChromaDB Vector Store (Qwen Embeddings)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Complete Workflow (main.py)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Phase 1: PDF â†’ VLM â†’ Extract â†’ Plan      â”‚  â”‚
â”‚  â”‚ Phase 2: Generate Multimodal Embeddings  â”‚  â”‚
â”‚  â”‚ Phase 2: Ingest into Vector Store        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â¸ï¸ Checkpointing: Resume on interruption       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
Lego_Assembly/
â”œâ”€â”€ main.py                      # Complete workflow orchestrator (Phase 1 + 2)
â”œâ”€â”€ ENV_TEMPLATE.txt             # Environment configuration template
â”œâ”€â”€ pyproject.toml               # Python dependencies (uv)
â”œâ”€â”€ QUICK_START.md               # Quick setup guide
â”‚
â”œâ”€â”€ src/                         # Phase 1: Manual Processing
â”‚   â”œâ”€â”€ api/                    # VLM clients (Qwen, DeepSeek, Kimi)
â”‚   â”œâ”€â”€ vision_processing/      # PDF extraction & VLM analysis
â”‚   â”œâ”€â”€ plan_generation/        # 3D planning, part database & graph builder
â”‚   â”‚   â””â”€â”€ graph_builder.py   # ğŸ§  NEW: Hierarchical graph construction
â”‚   â””â”€â”€ utils/                  # Configuration & caching
â”‚
â”œâ”€â”€ backend/                     # Phase 2: Vision-Enhanced RAG
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application (20+ endpoints)
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration
â”‚   â”‚   â”œâ”€â”€ models/            # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ llm/               # LLM clients (Qwen/DeepSeek/Moonshot)
â”‚   â”‚   â”œâ”€â”€ graph/             # ğŸ§  NEW: Hierarchical graph module
â”‚   â”‚   â”‚   â”œâ”€â”€ graph_manager.py       # Graph query interface
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py            # Module exports
â”‚   â”‚   â”œâ”€â”€ vision/            # â­ Vision analysis module
â”‚   â”‚   â”‚   â”œâ”€â”€ state_analyzer.py      # VLM-based part & subassembly detection
â”‚   â”‚   â”‚   â”œâ”€â”€ state_comparator.py    # Progress mapping
â”‚   â”‚   â”‚   â””â”€â”€ guidance_generator.py  # Next-step guidance
â”‚   â”‚   â”œâ”€â”€ vector_store/      # ChromaDB integration
â”‚   â”‚   â”œâ”€â”€ ingestion/         # Data processing
â”‚   â”‚   â”œâ”€â”€ rag/               # RAG pipeline (graph + vector hybrid)
â”‚   â”‚   â””â”€â”€ scripts/           # CLI tools
â”‚   â””â”€â”€ chroma_db/             # Vector database (auto-created)
â”‚
â”œâ”€â”€ frontend/                    # Phase 3: Enhanced Web UI
â”‚   â”œâ”€â”€ app/                   # Next.js pages (with vision tabs)
â”‚   â”œâ”€â”€ components/            # React components
â”‚   â”‚   â”œâ”€â”€ ImageUpload.tsx          # â­ NEW: Photo upload
â”‚   â”‚   â”œâ”€â”€ VisualGuidance.tsx       # â­ NEW: Analysis display
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx        # Text chat
â”‚   â”‚   â”œâ”€â”€ ManualSelector.tsx       # Manual selection
â”‚   â”‚   â””â”€â”€ StepNavigator.tsx        # Step navigation
â”‚   â””â”€â”€ lib/                   # API client & state (vision APIs)
â”‚
â”œâ”€â”€ output/                      # Generated outputs
â”‚   â”œâ”€â”€ {manual_id}_extracted.json     # Step extraction data
â”‚   â”œâ”€â”€ {manual_id}_plan.json          # 3D assembly plan
â”‚   â”œâ”€â”€ {manual_id}_dependencies.json  # Dependency graph
â”‚   â”œâ”€â”€ {manual_id}_graph.json         # ğŸ§  NEW: Hierarchical assembly graph
â”‚   â””â”€â”€ temp_pages/*.png               # Step images
â”‚
â””â”€â”€ data/
    â””â”€â”€ parts_database.db       # LEGO parts cache
```

---

## ğŸš€ Quick Start

See [QUICK_START.md](QUICK_START.md) for detailed setup instructions.

### Prerequisites

- Python 3.9+ with `uv` package manager
- Node.js 18+ and npm
- Poppler (for PDF processing)
- At least one API key: Qwen (DashScope), DeepSeek, or Moonshot

### 1. Configure Environment

```bash
# Copy template and add your API keys
cp ENV_TEMPLATE.txt .env
nano .env  # Add your API keys
```

**Required**: At least one of:
- `DASHSCOPE_API_KEY` - For Qwen-VL and Qwen-Max
- `DEEPSEEK_API_KEY` - For DeepSeek-Chat
- `MOONSHOT_API_KEY` - For Moonshot/Kimi

### 2. Install Dependencies

```bash
# Install with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

### 3. Process a Manual (Complete Workflow - Phase 1 + 2)

```bash
# Process LEGO manual from URL (automatically extracts AND ingests)
uv run python main.py https://www.lego.com/cdn/product-assets/product.bi.core.pdf/6454922.pdf

# NEW: 7-Step Enhanced Workflow
# âœ“ Step 1: Extract pages from PDF
# âœ“ Step 2: Analyze document & filter pages (Phase 0) â†’ User confirmation
# âœ“ Step 3: Context-aware VLM extraction (Phase 1)
# âœ“ Step 4: Build dependency graph
# âœ“ Step 5: Build hierarchical graph (enhanced with context hints)
# âœ“ Step 6: Generate 3D assembly plans
# âœ“ Step 7: Ingest into vector store with multimodal embeddings
# â±ï¸  Takes ~2-5 minutes total (depends on manual size)

# Output created in ./output/:
#   6454922_extracted.json         (Step data with subassembly_hints & context_references)
#   6454922_plan.json              (3D assembly plan)
#   6454922_dependencies.json      (Step dependencies)
#   6454922_graph.json             (Hierarchical structure - enhanced)
#   6454922_plan.txt               (Human-readable plan)
#   temp_pages/*.png               (Step diagram images)
#   .6454922_checkpoint.json       (Progress checkpoint - hidden)
```

**ğŸ¯ One Command Does It All**: No need to run separate extraction and ingestion steps!

**ğŸ“„ Smart Document Filtering (Phase 0)**:
- Analyzes PDF structure automatically
- Filters out 10-15% irrelevant pages (covers, ads, inventory)
- Asks for user confirmation before processing
- Saves API calls and processing time

**ğŸ§  Context-Aware Extraction (Phase 1)**:
- Maintains sliding window of last 2 steps for immediate context
- Tracks long-term build state (completed subassemblies, current work)
- Generates enhanced subassembly hints (20-30% better detection)
- Captures inter-step references automatically

**â¸ï¸ Checkpointing Support**: If interrupted (Ctrl+C or crash), simply rerun the same command to resume:
- âœ“ Completed steps are automatically skipped
- âœ“ Progress is saved after each major step
- âœ“ No wasted computation or API calls

**Advanced Options**:
```bash
# Skip vector store ingestion (only generate files)
uv run python main.py <url> --skip-ingestion

# Use text-only embeddings (faster, less accurate)
uv run python main.py <url> --no-multimodal

# Start from scratch (ignore checkpoint)
uv run python main.py <url> --no-resume

# Disable spatial features (see Comparison Testing below)
uv run python main.py <url> --no-spatial

# View all options
uv run python main.py --help
```

### 3b. Comparison Testing: Spatial Features A/B Testing

**ğŸ”¬ NEW: Compare manual processing with/without spatial features**

The `--no-spatial` flag disables all spatial processing features:
- **Spatial Relationships**: VLM-extracted 3D positioning data (position, rotation, alignment)
- **Spatial-Temporal Patterns**: Post-processing pattern analysis (progressive sequences)

**Use Case**: Ingest the same manual with different configurations to compare:
- Graph structure complexity
- Subassembly detection accuracy
- Query retrieval performance
- RAG response quality
- VLM token usage and processing speed

**Step-by-Step Comparison Workflow**:

```bash
# 1. Full Features (Baseline)
uv run python main.py manual.pdf \
  -o output/baseline \
  --assembly-id manual_baseline

# Configuration in output/baseline/manual_baseline_graph_summary.txt:
#   Spatial Relationships: Enabled
#   Spatial-Temporal Patterns: Enabled

# 2. No Spatial Features (Comparison)
uv run python main.py manual.pdf \
  -o output/no_spatial \
  --assembly-id manual_no_spatial \
  --no-spatial

# VLM extraction skips spatial data (saves ~10-15% tokens)
# Spatial reasoning uses default positions only
# Post-processing pattern detection is disabled
# Fastest processing, simplest graph structure
```

**ğŸ¯ Both Versions Now in Vector Store!**

Each version has a unique `manual_id` and coexists in ChromaDB. You can query them separately:

```bash
# Start backend
cd backend
uvicorn app.main:app --reload --port 8000

# Query each version
curl -X POST http://localhost:8000/api/query/text \
  -H "Content-Type: application/json" \
  -d '{
    "manual_id": "manual_baseline",
    "question": "How to attach the wheels?"
  }'

curl -X POST http://localhost:8000/api/query/text \
  -H "Content-Type: application/json" \
  -d '{
    "manual_id": "manual_no_spatial",
    "question": "How to attach the wheels?"
  }'
```

**ğŸ“Š Compare Results**:

```bash
# 1. Compare Graph Structures
diff output/baseline/manual_baseline_graph_summary.txt \
     output/no_spatial/manual_no_spatial_graph_summary.txt

# Look for:
#   - Subassembly count differences
#   - Pattern detection differences
#   - Graph depth/complexity

# 2. Compare Graph JSON
python3 -c "
import json
baseline = json.load(open('output/baseline/manual_baseline_graph.json'))
no_spatial = json.load(open('output/no_spatial/manual_no_spatial_graph.json'))

print('Baseline subassemblies:', baseline['metadata']['total_subassemblies'])
print('No-spatial subassemblies:', no_spatial['metadata']['total_subassemblies'])
print('Baseline config:', baseline['metadata']['configuration'])
print('No-spatial config:', no_spatial['metadata']['configuration'])
"

# 3. Test Query Performance (via frontend or API)
# - Load each manual in frontend
# - Ask identical questions
# - Compare response accuracy and relevance

# 4. Analyze Processing Time/Cost
# Check console output for:
#   - VLM token usage (baseline vs no_spatial)
#   - Processing duration
#   - Graph building time
```

**ğŸ’¡ Expected Differences**:

| Aspect | Baseline (Spatial Enabled) | No Spatial (--no-spatial) |
|--------|---------------------------|---------------------------|
| VLM Tokens | 100% | ~85-90% (saves 10-15%) |
| Spatial Data | Full 3D coordinates | Default positions only |
| Subassemblies | VLM detection | VLM detection |
| Patterns Detected | Yes (spatial-temporal) | No |
| Processing Time | Longer | Faster |
| Graph Complexity | Higher (with patterns) | Lower (simpler) |
| Query Accuracy | Best for spatial queries | Better for structural queries |

**ğŸ¯ When to Use Each Mode**:

- **Baseline (default)**: Production use, best overall accuracy, spatial reasoning needed
- **No Spatial (--no-spatial)**: Faster processing, simpler graphs, when manual has poor/unclear spatial info, resource-constrained environments, or for comparison testing

### 4. Start RAG Backend

```bash
# Navigate to backend
cd backend

# Start FastAPI server
uvicorn app.main:app --reload --port 8000

# Access at: http://localhost:8000
# API docs: http://localhost:8000/docs
```

**Note**: Manual is already ingested from Step 3! Just start the backend and you're ready to query.

### 5. Start Frontend

```bash
# Navigate to frontend (new terminal)
cd frontend

# Install dependencies
npm install

# Configure backend URL
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > .env.local

# Start development server
npm run dev

# Access at: http://localhost:3000
```

---

## ğŸ“š Features & Capabilities

### ğŸ†• Vision-Based Assembly State Analysis

**The Star Feature**: Upload photos of your physical assembly and get intelligent, state-aware guidance!

**How It Works**:
1. **Upload 2-4 Photos**: Take pictures from different angles (front, back, sides, top)
2. **VLM Analysis**: Qwen-VL analyzes images to detect parts, colors, and connections
3. **State Comparison**: System compares detected state with Phase 1 plan data
4. **Progress Mapping**: Determines which steps are completed (e.g., "Steps 1-5 done, currently on step 6")
5. **Error Detection**: Identifies missing parts or incorrect placements
6. **Guidance Generation**: Provides clear next-step instructions with reference images

**Example Workflow**:
```
1. User builds steps 1-3 of LEGO set
2. Takes 3 photos from different angles
3. Uploads to system via frontend
4. System responds:
   âœ… Progress: 15% (3/20 steps completed)
   âœ… Current State: Yellow base plate with 4 red bricks attached
   âš ï¸  Missing: 1 blue 2x2 brick for step 3
   ğŸ“‹ Next Step: Attach green 2x4 brick to right side
   ğŸ–¼ï¸  [Shows reference image for step 4]
```

**Key Capabilities**:
- âœ… Multi-view image analysis (2-4 photos)
- âœ… Automatic part detection (colors, shapes, part IDs)
- âœ… Progress percentage calculation
- âœ… Error detection and correction suggestions
- âœ… Next-step prediction with visual references
- âœ… Iterative workflow: build â†’ photo â†’ guidance â†’ repeat

**API Endpoints**:
- `POST /api/vision/upload-images` - Upload assembly photos
- `POST /api/vision/analyze` - Analyze state and generate guidance
- `DELETE /api/vision/session/{session_id}` - Cleanup session

---

### ğŸ†• Multimodal Query System

**The Enhanced Feature**: Ask vague questions with your assembly photos and get accurate, context-aware answers!

**How It Works**:
1. **Upload Photos**: Same as vision analysis - 2-4 photos from different angles
2. **Ask Vague Questions**: "What's next?", "Help!", "Am I doing this right?"
3. **Automatic Context**: System analyzes images, detects parts, estimates current step
4. **Query Augmentation**: LLM expands vague query with visual context
5. **Smart Retrieval**: Searches for steps matching detected parts
6. **Accurate Answer**: Gets precise guidance without requiring step numbers

**Example Workflow**:
```
Traditional (without multimodal):
User: "What's next?"
System: âŒ "I need more context. Which step are you on?"

With Multimodal RAG:
User: "What's next?" + [uploads 3 photos]
System: 
  1. Detects: red 2x4 brick, blue plate, yellow base
  2. Estimates: Steps 17-19
  3. Augments query: "What is the next step after steps 17-19 using red 2x4 brick?"
  4. Retrieves: Step 18 with 92% confidence
  5. Answers: âœ… "Based on your assembly, attach the yellow 1x2 plate to the top-right corner..."
```

**Key Features**:
- âœ… **Vague Query Understanding**: No need to specify step numbers
- âœ… **Image-Aware Retrieval**: Boosts results matching your detected parts
- âœ… **Automatic Step Estimation**: Infers where you are from photos
- âœ… **Query Expansion**: Turns "Help!" into specific, searchable queries
- âœ… **Session-Based**: Upload once, ask multiple questions

**API Endpoints**:
- `POST /api/query/multimodal` - Query with uploaded images
- `POST /api/query/text` - Text query (optionally with session_id)

**Usage Example**:
```bash
# 1. Upload images
curl -X POST http://localhost:8000/api/vision/upload-images \
  -F "images=@photo1.jpg" -F "images=@photo2.jpg"
# Returns: {"session_id": "abc-123", ...}

# 2. Ask vague question with context
curl -X POST http://localhost:8000/api/query/multimodal \
  -d '{
    "manual_id": "6454922",
    "question": "What should I do next?",
    "session_id": "abc-123"
  }'
```

**Re-Ingestion for Best Results**:
```bash
# Delete old embeddings
curl -X DELETE http://localhost:8000/api/manual/6454922

# Re-ingest with multimodal embeddings
curl -X POST http://localhost:8000/api/ingest/manual/6454922
```

This generates fused embeddings (text + diagram descriptions) for improved visual retrieval.

---

### ğŸ§  Hierarchical Assembly Graph (NEW!)

**The Mental Model Feature**: System now understands assembly structure beyond individual steps!

**How It Works**:
1. **Graph Construction**: During manual processing (Phase 1), builds hierarchical graph:
   - **Parts**: Individual LEGO pieces with roles (structural, decorative, functional)
   - **Subassemblies**: Groups of parts forming distinct units (e.g., "wheel assembly")
   - **Model**: Final complete assembly
2. **Relationship Tracking**: Understands parent-child connections (what becomes part of what)
3. **State Progression**: Tracks assembly state at each step (what exists, what's new)
4. **Graph-Enhanced Retrieval**: Combines structural and semantic search for better answers

**Key Capabilities**:
- âœ… **Structural Queries**: "What subassembly contains the red brick?"
- âœ… **Relationship Queries**: "What is the wheel assembly made from?"
- âœ… **Hierarchy Visualization**: "Show me the structure of step 5"
- âœ… **Progress Tracking**: Understands completion via subassemblies
- âœ… **Subassembly Detection**: Recognizes completed structures from photos

**Example Workflow**:
```
1. Process manual â†’ Generates graph.json
2. System detects subassemblies (e.g., "Front Wheel Assembly" at step 5)
3. User uploads photo â†’ System recognizes "Front Wheel Assembly" 
4. System estimates: "You've completed steps 1-5 (25%)"
5. User asks: "What is the wheel assembly for?"
6. System uses graph: "The Front Wheel Assembly (created in step 5) 
   contains 2 wheels, 1 axle, and 1 hub. It attaches to the Car Body 
   in step 12."
```

**API Endpoints**:
- `GET /api/manual/{id}/graph/summary` - Graph statistics
- `GET /api/manual/{id}/graph/subassemblies` - List all subassemblies
- `GET /api/manual/{id}/graph/step/{n}/state` - State at specific step
- `GET /api/manual/{id}/progress` - Assembly progress (with photo support)
- `GET /api/manual/{id}/graph/node/{id}/hierarchy` - Hierarchical path

**Technical Details**: See [IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md)

---

### Phase 1: Manual Processing

**Input**: PDF instruction manual or image directory
**Output**: Structured JSON + step images (used as ground truth for vision analysis)

- **Phase 0 - Document Understanding** (NEW):
  - Analyzes PDF structure with VLM sampling
  - Classifies pages: instruction, cover, inventory, advertisement
  - Filters to only relevant instruction pages
  - User confirmation before processing
  - **Benefit**: 10-15% fewer API calls

- **Phase 1 - Context-Aware Extraction** (NEW):
  - **Sliding Window Memory**: Tracks last 2 steps (~600 tokens)
  - **Long-Term Memory**: Maintains overall build state (~500 tokens)
  - **Enhanced Prompts**: Includes context in VLM requests
  - **Subassembly Hints**: Detects new subassemblies during extraction
  - **Context References**: Captures inter-step dependencies
  - **Benefit**: 20-30% better subassembly detection, improved accuracy

- **VLM Extraction**: Uses Qwen-VL, DeepSeek, or Kimi to analyze manual pages
- **Step Detection**: Automatically identifies step boundaries
- **Part Recognition**: Extracts part descriptions, colors, and quantities
- **Spatial Analysis**: Determines 3D positions and relationships
- **Dependency Graph**: Builds assembly order with parallel paths
- **Part Matching**: Integrates with Rebrickable for part IDs

**Example Usage**:
```bash
python main.py <manual_url> -o ./output --assembly-id 6454922
```

### Phase 2: RAG Backend

**Input**: User questions about assembly  
**Output**: Context-aware AI responses

- **Semantic Search**: ChromaDB with Qwen text-embedding-v2
- **Data Ingestion**: Processes Phase 1 outputs into vector store
- **Hybrid Retrieval**: Combines vector similarity with metadata filtering
- **Multi-LLM Support**: 
  - Qwen-Max (primary, best for Chinese)
  - DeepSeek-Chat (cost-effective)
  - Moonshot (strong context understanding)
- **Context Assembly**: Retrieves relevant steps, parts, and dependencies
- **REST API**: FastAPI with automatic OpenAPI docs

**Supported Queries**:
- "What do I do in step 5?"
- "What parts do I need?"
- "How do I attach the red brick?"
- "What's the next step?"
- "Show me dependencies for step 10"

**API Endpoints**:
```
GET  /health                                    # Health check
POST /api/ingest/manual/{id}                    # Ingest manual with multimodal embeddings
POST /api/query/text                            # Ask question (optionally with session_id)
POST /api/query/multimodal                      # Query with uploaded images
GET  /api/manual/{id}/step/{num}                # Get step details
GET  /api/manuals                               # List all manuals
GET  /api/manual/{id}/steps                     # List manual steps
GET  /api/image?path={path}                     # Serve images
POST /api/vision/upload-images                  # Upload assembly photos
POST /api/vision/analyze                        # Analyze assembly state
DELETE /api/vision/session/{id}                 # Cleanup session

# ğŸ§  NEW: Hierarchical Graph Endpoints
GET  /api/manual/{id}/graph/summary             # Graph statistics
GET  /api/manual/{id}/graph/step/{n}/state      # Assembly state at step
GET  /api/manual/{id}/graph/subassemblies       # List all subassemblies
GET  /api/manual/{id}/progress                  # Assembly progress tracking
GET  /api/manual/{id}/graph/node/{id}/hierarchy # Node hierarchy path
```

### Phase 3: Frontend UI

**Access**: http://localhost:3000

- **Manual Selector**: Browse and select from ingested manuals
- **Step Navigator**: 
  - View current step with image
  - Navigate with Previous/Next buttons
  - Progress bar showing completion
  - Parts list for current step
- **AI Chat Interface**:
  - Natural language questions
  - Real-time responses from RAG backend
  - Message history
  - Quick question shortcuts
- **LEGO Theme**: Custom Tailwind CSS with LEGO colors
- **Responsive Design**: Works on desktop and mobile

---

## âš™ï¸ Configuration

### Environment Variables

All configuration in `.env` file (copy from `ENV_TEMPLATE.txt`):

```bash
# VLM API Keys (Phase 1 & RAG LLM)
DASHSCOPE_API_KEY=sk-...          # Alibaba Cloud for Qwen
DEEPSEEK_API_KEY=sk-...            # DeepSeek
MOONSHOT_API_KEY=sk-...            # Moonshot/Kimi

# RAG System Settings
RAG_LLM_PROVIDER=qwen              # qwen, deepseek, or moonshot
RAG_LLM_MODEL=qwen-max             # Model name
RAG_EMBEDDING_PROVIDER=qwen        # Only qwen supported for embeddings
RAG_EMBEDDING_MODEL=text-embedding-v2

# Vector Database
CHROMA_PERSIST_DIR=./backend/chroma_db
COLLECTION_NAME=lego_manuals

# Server Settings
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO

# Data Paths
OUTPUT_DIR=./output
TEMP_PAGES_DIR=./output/temp_pages

# RAG Pipeline
TOP_K_RESULTS=5
SIMILARITY_THRESHOLD=0.7
MAX_CONTEXT_LENGTH=4000

# Part Database
PARTS_DB_PATH=./data/parts_database.db
REBRICKABLE_API_KEY=...           # Optional, for part enrichment
```

### LLM Provider Selection

**Qwen (Recommended)**:
- Best bilingual (Chinese/English) support
- Native embedding model included
- Cost: ~Â¥0.12/1K tokens (~$0.017)

**DeepSeek**:
- Cost-effective alternative
- Good performance
- Cost: ~Â¥0.01/1K tokens (~$0.0014)
- Note: No native embedding API (uses Qwen for embeddings)

**Moonshot (Kimi)**:
- Strong context understanding
- 32K context window
- Cost: ~Â¥0.12/1K tokens (~$0.017)
- Note: Uses Qwen for embeddings

---

## ğŸ”§ Development

### Phase 1: Adding New VLM

1. Create client in `src/api/new_vlm.py`
2. Add to `VLMStepExtractor` in `src/vision_processing/vlm_step_extractor.py`
3. Update `ENV_TEMPLATE.txt` with new API key

### Phase 2: Customizing RAG

**Modify Prompts**: Edit `backend/app/rag/generator.py`
```python
system_prompt = """Your custom system prompt here..."""
```

**Adjust Retrieval**: Edit `backend/app/rag/retrieval.py`
```python
top_k = 10  # Retrieve more results
similarity_threshold = 0.8  # Stricter threshold
```

**Add New Endpoint**: Edit `backend/app/main.py`
```python
@app.get("/api/custom")
async def custom_endpoint():
    return {"data": "your response"}
```

### Phase 3: Frontend Customization

**Change Theme**: Edit `frontend/tailwind.config.ts`
```typescript
colors: {
  lego: {
    red: '#D01012',    // Your custom colors
    // ...
  }
}
```

**Add Component**: Create in `frontend/components/`
```typescript
export default function NewComponent() {
  // Your component logic
}
```

---

## ğŸ“Š Performance & Costs

### Processing Speed
- **Phase 1**: ~10-20 seconds for 20-step manual
- **Phase 2 Ingestion**: ~30 seconds for 50-step manual
- **Phase 2 Query**: 1-3 seconds (first query), <1s (cached)
- **Phase 3 Load**: <1 second

### API Costs (Qwen)
- **Manual Processing**: ~Â¥5-10 per manual (~$0.70-1.40)
- **Embedding**: ~Â¥0.0001 per text (~$0.000014)
- **Query**: ~Â¥0.1-0.5 per query (~$0.014-0.070)

### Storage
- **Vector DB**: ~10MB per manual
- **Image Cache**: ~50MB per manual
- **Part Database**: ~5MB (shared)

---

## ğŸ› ï¸ Utility Scripts

The system includes several utility scripts for debugging and analysis:

### Graph Visualization
```bash
# Visualize hierarchical assembly graph
python3 backend/app/scripts/visualize_graph.py output/6454922_graph.json

# With detailed subassembly information
python3 backend/app/scripts/visualize_graph.py output/6454922_graph.json --show-subassemblies

# With step progression
python3 backend/app/scripts/visualize_graph.py output/6454922_graph.json --show-steps 10
```

### Graph Testing
```bash
# Test graph regeneration with enhanced implementation
python3 backend/app/scripts/test_graph_regeneration.py
```

### Database Inspection
```bash
# View vector store contents
cd backend
python -m app.scripts.inspect_vector_store

# View extracted steps
python -m app.scripts.view_steps 6454922

# Simple database inspection
python -m app.scripts.inspect_db_simple
```

### Batch Processing
```bash
# Ingest all manuals
cd backend
python -m app.scripts.ingest_all
```

For detailed graph debugging and verification, see the **Graph Visualization & Debugging** section in [QUICK_START.md](QUICK_START.md).

---

## ğŸ› Troubleshooting

### "API key not found"
```bash
# Check .env file exists
cat .env | grep API_KEY

# Make sure you copied from template
cp ENV_TEMPLATE.txt .env
```

### "No manuals available" (Frontend)
```bash
# Ingest manual first
cd backend
python -m app.scripts.ingest_manual 6454922
```

### "Cannot connect to backend" (Frontend)
```bash
# Check backend is running
curl http://localhost:8000/health

# Check frontend .env.local
cat frontend/.env.local
```

### "Module not found" errors
```bash
# Reinstall dependencies
uv sync

# Or with pip
pip install -e .
```

### Port already in use
```bash
# Find and kill process
lsof -i :8000  # Backend
lsof -i :3000  # Frontend
kill -9 <PID>
```

---

## ğŸ“– Documentation

- **[QUICK_START.md](QUICK_START.md)** - Step-by-step setup guide
- **[IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)** - Technical details
- **[MULTIMODAL_IMPLEMENTATION_SUMMARY.md](MULTIMODAL_IMPLEMENTATION_SUMMARY.md)** - Multimodal RAG technical documentation
- **[MENTAL_MODEL_IMPLEMENTATION_PLAN.md](MENTAL_MODEL_IMPLEMENTATION_PLAN.md)** - Hierarchical graph design document
- **[IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md)** - Graph implementation summary
- **API Docs**: http://localhost:8000/docs (when backend running)

---

## ğŸ¯ Roadmap

### Current Features
- âœ… **Phase 0: Document Understanding** (intelligent page filtering)
- âœ… **Phase 1: Context-Aware Extraction** (sliding window + long-term memory)
- âœ… Multi-VLM manual processing
- âœ… 3D plan generation
- âœ… **Hierarchical assembly graph** (parts â†’ subassemblies â†’ model, enhanced with context)
- âœ… Multimodal embeddings (text + diagrams)
- âœ… LLM query augmentation for vague queries
- âœ… **Graph-enhanced retrieval** (structural + semantic search)
- âœ… RAG pipeline with semantic search
- âœ… Image upload for visual queries
- âœ… Part recognition with computer vision
- âœ… **Subassembly detection** from user photos
- âœ… Progress tracking with VLM and graph
- âœ… Web UI with chat interface
- âœ… Chinese LLM support
- âœ… **Enhanced subassembly hints** (20-30% better accuracy)
- âœ… **Inter-step reference tracking** (context-aware dependencies)

### Planned Enhancements
- [ ] Progress saving and resume
- [ ] Multi-manual comparison
- [ ] AR overlay guidance
- [ ] Voice interface
- [ ] 3D visualization
- [ ] Community features
- [ ] Mobile app
- [ ] Offline mode

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- **LEGOÂ®** is a trademark of the LEGO Group
- VLM Providers: Alibaba Cloud (Qwen), DeepSeek, Moonshot AI
- Vector DB: ChromaDB
- Frameworks: FastAPI, Next.js, React

---

## ğŸ“® Support

For issues or questions:
1. Check [QUICK_START.md](QUICK_START.md) troubleshooting section
2. Review API documentation at `/docs` endpoint
3. Check configuration in `.env` file

**System Version**: 2.2.0 (with Context-Aware Processing)
**Last Updated**: January 2026
**Status**: âœ… Production Ready
